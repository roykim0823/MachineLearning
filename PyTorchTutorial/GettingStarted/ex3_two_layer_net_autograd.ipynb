{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensors and autograd\n",
    "-------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Tensors, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "\n",
    "A PyTorch Tensor represents a node in a computational graph. If ``x`` is a\n",
    "Tensor that has ``x.requires_grad=True`` then ``x.grad`` is another Tensor\n",
    "holding the gradient of ``x`` with respect to some scalar value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\t23547240.000000\n",
      "   1\t16368425.000000\n",
      "   2\t13744135.000000\n",
      "   3\t13129416.000000\n",
      "   4\t13335319.000000\n",
      "   5\t13551493.000000\n",
      "   6\t13166821.000000\n",
      "   7\t11929022.000000\n",
      "   8\t9944028.000000\n",
      "   9\t7674540.500000\n",
      "  10\t5551736.000000\n",
      "  11\t3845173.500000\n",
      "  12\t2603190.000000\n",
      "  13\t1760653.500000\n",
      "  14\t1208902.625000\n",
      "  15\t853298.750000\n",
      "  16\t623273.375000\n",
      "  17\t472582.937500\n",
      "  18\t371278.718750\n",
      "  19\t300988.062500\n",
      "  20\t250436.015625\n",
      "  21\t212696.187500\n",
      "  22\t183497.500000\n",
      "  23\t160193.828125\n",
      "  24\t141101.015625\n",
      "  25\t125113.835938\n",
      "  26\t111497.757812\n",
      "  27\t99757.968750\n",
      "  28\t89540.351562\n",
      "  29\t80586.312500\n",
      "  30\t72688.867188\n",
      "  31\t65693.945312\n",
      "  32\t59473.828125\n",
      "  33\t53922.578125\n",
      "  34\t48958.710938\n",
      "  35\t44510.042969\n",
      "  36\t40514.101562\n",
      "  37\t36914.261719\n",
      "  38\t33670.683594\n",
      "  39\t30745.968750\n",
      "  40\t28102.492188\n",
      "  41\t25708.359375\n",
      "  42\t23536.910156\n",
      "  43\t21564.917969\n",
      "  44\t19772.941406\n",
      "  45\t18143.048828\n",
      "  46\t16663.730469\n",
      "  47\t15315.412109\n",
      "  48\t14085.493164\n",
      "  49\t12961.746094\n",
      "  50\t11934.931641\n",
      "  51\t10999.587891\n",
      "  52\t10144.725586\n",
      "  53\t9361.490234\n",
      "  54\t8643.085938\n",
      "  55\t7984.049805\n",
      "  56\t7378.511719\n",
      "  57\t6822.371094\n",
      "  58\t6311.135254\n",
      "  59\t5841.054199\n",
      "  60\t5408.316406\n",
      "  61\t5009.619629\n",
      "  62\t4642.570312\n",
      "  63\t4304.041016\n",
      "  64\t3991.765625\n",
      "  65\t3703.687256\n",
      "  66\t3437.680908\n",
      "  67\t3191.940430\n",
      "  68\t2964.758057\n",
      "  69\t2754.717285\n",
      "  70\t2560.320557\n",
      "  71\t2380.369141\n",
      "  72\t2213.851562\n",
      "  73\t2059.601074\n",
      "  74\t1916.639648\n",
      "  75\t1784.129395\n",
      "  76\t1661.286987\n",
      "  77\t1547.342163\n",
      "  78\t1441.513306\n",
      "  79\t1343.296387\n",
      "  80\t1252.072144\n",
      "  81\t1167.327271\n",
      "  82\t1088.686157\n",
      "  83\t1015.566895\n",
      "  84\t947.601685\n",
      "  85\t884.377747\n",
      "  86\t825.557861\n",
      "  87\t770.871460\n",
      "  88\t719.902954\n",
      "  89\t672.464478\n",
      "  90\t628.277832\n",
      "  91\t587.107300\n",
      "  92\t548.751831\n",
      "  93\t513.006653\n",
      "  94\t479.669250\n",
      "  95\t448.582001\n",
      "  96\t419.591431\n",
      "  97\t392.558563\n",
      "  98\t367.342224\n",
      "  99\t343.780029\n",
      " 100\t321.795227\n",
      " 101\t301.267792\n",
      " 102\t282.097595\n",
      " 103\t264.200470\n",
      " 104\t247.473328\n",
      " 105\t231.837738\n",
      " 106\t217.222610\n",
      " 107\t203.565903\n",
      " 108\t190.795929\n",
      " 109\t178.867569\n",
      " 110\t167.693420\n",
      " 111\t157.243561\n",
      " 112\t147.469482\n",
      " 113\t138.322678\n",
      " 114\t129.757690\n",
      " 115\t121.742424\n",
      " 116\t114.238235\n",
      " 117\t107.212341\n",
      " 118\t100.631851\n",
      " 119\t94.466705\n",
      " 120\t88.696579\n",
      " 121\t83.282921\n",
      " 122\t78.212166\n",
      " 123\t73.455902\n",
      " 124\t68.997826\n",
      " 125\t64.817993\n",
      " 126\t60.898605\n",
      " 127\t57.224140\n",
      " 128\t53.776981\n",
      " 129\t50.541920\n",
      " 130\t47.506523\n",
      " 131\t44.661667\n",
      " 132\t41.991997\n",
      " 133\t39.484238\n",
      " 134\t37.129818\n",
      " 135\t34.919640\n",
      " 136\t32.845055\n",
      " 137\t30.896610\n",
      " 138\t29.066496\n",
      " 139\t27.346756\n",
      " 140\t25.731716\n",
      " 141\t24.214201\n",
      " 142\t22.788982\n",
      " 143\t21.450186\n",
      " 144\t20.191181\n",
      " 145\t19.007788\n",
      " 146\t17.895302\n",
      " 147\t16.849571\n",
      " 148\t15.866377\n",
      " 149\t14.941917\n",
      " 150\t14.072420\n",
      " 151\t13.254770\n",
      " 152\t12.485591\n",
      " 153\t11.762005\n",
      " 154\t11.081457\n",
      " 155\t10.441877\n",
      " 156\t9.838587\n",
      " 157\t9.271498\n",
      " 158\t8.737329\n",
      " 159\t8.235473\n",
      " 160\t7.762322\n",
      " 161\t7.317136\n",
      " 162\t6.898046\n",
      " 163\t6.502788\n",
      " 164\t6.131147\n",
      " 165\t5.781552\n",
      " 166\t5.451830\n",
      " 167\t5.141836\n",
      " 168\t4.849521\n",
      " 169\t4.573972\n",
      " 170\t4.314525\n",
      " 171\t4.069844\n",
      " 172\t3.839315\n",
      " 173\t3.622022\n",
      " 174\t3.417544\n",
      " 175\t3.224682\n",
      " 176\t3.043254\n",
      " 177\t2.871550\n",
      " 178\t2.710324\n",
      " 179\t2.557729\n",
      " 180\t2.414217\n",
      " 181\t2.279019\n",
      " 182\t2.151472\n",
      " 183\t2.031107\n",
      " 184\t1.917635\n",
      " 185\t1.810437\n",
      " 186\t1.709575\n",
      " 187\t1.614045\n",
      " 188\t1.524317\n",
      " 189\t1.439605\n",
      " 190\t1.359504\n",
      " 191\t1.284088\n",
      " 192\t1.212811\n",
      " 193\t1.145661\n",
      " 194\t1.082268\n",
      " 195\t1.022363\n",
      " 196\t0.966012\n",
      " 197\t0.912638\n",
      " 198\t0.862306\n",
      " 199\t0.814912\n",
      " 200\t0.770058\n",
      " 201\t0.727748\n",
      " 202\t0.687722\n",
      " 203\t0.650017\n",
      " 204\t0.614344\n",
      " 205\t0.580622\n",
      " 206\t0.548913\n",
      " 207\t0.518921\n",
      " 208\t0.490525\n",
      " 209\t0.463761\n",
      " 210\t0.438425\n",
      " 211\t0.414610\n",
      " 212\t0.392035\n",
      " 213\t0.370673\n",
      " 214\t0.350516\n",
      " 215\t0.331476\n",
      " 216\t0.313520\n",
      " 217\t0.296514\n",
      " 218\t0.280484\n",
      " 219\t0.265265\n",
      " 220\t0.250895\n",
      " 221\t0.237343\n",
      " 222\t0.224532\n",
      " 223\t0.212396\n",
      " 224\t0.200933\n",
      " 225\t0.190142\n",
      " 226\t0.179942\n",
      " 227\t0.170253\n",
      " 228\t0.161138\n",
      " 229\t0.152474\n",
      " 230\t0.144282\n",
      " 231\t0.136535\n",
      " 232\t0.129256\n",
      " 233\t0.122319\n",
      " 234\t0.115807\n",
      " 235\t0.109609\n",
      " 236\t0.103788\n",
      " 237\t0.098219\n",
      " 238\t0.092981\n",
      " 239\t0.088039\n",
      " 240\t0.083353\n",
      " 241\t0.078931\n",
      " 242\t0.074735\n",
      " 243\t0.070761\n",
      " 244\t0.067026\n",
      " 245\t0.063445\n",
      " 246\t0.060103\n",
      " 247\t0.056918\n",
      " 248\t0.053907\n",
      " 249\t0.051075\n",
      " 250\t0.048384\n",
      " 251\t0.045825\n",
      " 252\t0.043404\n",
      " 253\t0.041135\n",
      " 254\t0.038973\n",
      " 255\t0.036930\n",
      " 256\t0.034992\n",
      " 257\t0.033161\n",
      " 258\t0.031434\n",
      " 259\t0.029795\n",
      " 260\t0.028242\n",
      " 261\t0.026760\n",
      " 262\t0.025377\n",
      " 263\t0.024056\n",
      " 264\t0.022798\n",
      " 265\t0.021623\n",
      " 266\t0.020483\n",
      " 267\t0.019433\n",
      " 268\t0.018424\n",
      " 269\t0.017468\n",
      " 270\t0.016562\n",
      " 271\t0.015708\n",
      " 272\t0.014900\n",
      " 273\t0.014144\n",
      " 274\t0.013422\n",
      " 275\t0.012731\n",
      " 276\t0.012089\n",
      " 277\t0.011479\n",
      " 278\t0.010889\n",
      " 279\t0.010340\n",
      " 280\t0.009815\n",
      " 281\t0.009319\n",
      " 282\t0.008851\n",
      " 283\t0.008407\n",
      " 284\t0.007986\n",
      " 285\t0.007588\n",
      " 286\t0.007216\n",
      " 287\t0.006851\n",
      " 288\t0.006513\n",
      " 289\t0.006202\n",
      " 290\t0.005895\n",
      " 291\t0.005607\n",
      " 292\t0.005336\n",
      " 293\t0.005075\n",
      " 294\t0.004834\n",
      " 295\t0.004597\n",
      " 296\t0.004377\n",
      " 297\t0.004166\n",
      " 298\t0.003968\n",
      " 299\t0.003782\n",
      " 300\t0.003605\n",
      " 301\t0.003436\n",
      " 302\t0.003278\n",
      " 303\t0.003126\n",
      " 304\t0.002982\n",
      " 305\t0.002846\n",
      " 306\t0.002715\n",
      " 307\t0.002595\n",
      " 308\t0.002478\n",
      " 309\t0.002368\n",
      " 310\t0.002261\n",
      " 311\t0.002160\n",
      " 312\t0.002065\n",
      " 313\t0.001974\n",
      " 314\t0.001888\n",
      " 315\t0.001806\n",
      " 316\t0.001728\n",
      " 317\t0.001657\n",
      " 318\t0.001587\n",
      " 319\t0.001522\n",
      " 320\t0.001461\n",
      " 321\t0.001400\n",
      " 322\t0.001344\n",
      " 323\t0.001289\n",
      " 324\t0.001236\n",
      " 325\t0.001184\n",
      " 326\t0.001136\n",
      " 327\t0.001092\n",
      " 328\t0.001050\n",
      " 329\t0.001010\n",
      " 330\t0.000971\n",
      " 331\t0.000934\n",
      " 332\t0.000900\n",
      " 333\t0.000865\n",
      " 334\t0.000833\n",
      " 335\t0.000802\n",
      " 336\t0.000773\n",
      " 337\t0.000744\n",
      " 338\t0.000717\n",
      " 339\t0.000691\n",
      " 340\t0.000667\n",
      " 341\t0.000644\n",
      " 342\t0.000622\n",
      " 343\t0.000600\n",
      " 344\t0.000580\n",
      " 345\t0.000561\n",
      " 346\t0.000541\n",
      " 347\t0.000524\n",
      " 348\t0.000506\n",
      " 349\t0.000489\n",
      " 350\t0.000473\n",
      " 351\t0.000457\n",
      " 352\t0.000442\n",
      " 353\t0.000428\n",
      " 354\t0.000415\n",
      " 355\t0.000402\n",
      " 356\t0.000388\n",
      " 357\t0.000378\n",
      " 358\t0.000366\n",
      " 359\t0.000354\n",
      " 360\t0.000344\n",
      " 361\t0.000334\n",
      " 362\t0.000324\n",
      " 363\t0.000314\n",
      " 364\t0.000305\n",
      " 365\t0.000296\n",
      " 366\t0.000287\n",
      " 367\t0.000280\n",
      " 368\t0.000272\n",
      " 369\t0.000264\n",
      " 370\t0.000257\n",
      " 371\t0.000250\n",
      " 372\t0.000243\n",
      " 373\t0.000236\n",
      " 374\t0.000230\n",
      " 375\t0.000224\n",
      " 376\t0.000218\n",
      " 377\t0.000212\n",
      " 378\t0.000207\n",
      " 379\t0.000201\n",
      " 380\t0.000196\n",
      " 381\t0.000192\n",
      " 382\t0.000187\n",
      " 383\t0.000182\n",
      " 384\t0.000178\n",
      " 385\t0.000173\n",
      " 386\t0.000169\n",
      " 387\t0.000165\n",
      " 388\t0.000160\n",
      " 389\t0.000157\n",
      " 390\t0.000153\n",
      " 391\t0.000149\n",
      " 392\t0.000146\n",
      " 393\t0.000142\n",
      " 394\t0.000139\n",
      " 395\t0.000136\n",
      " 396\t0.000133\n",
      " 397\t0.000130\n",
      " 398\t0.000127\n",
      " 399\t0.000123\n",
      " 400\t0.000121\n",
      " 401\t0.000119\n",
      " 402\t0.000116\n",
      " 403\t0.000113\n",
      " 404\t0.000111\n",
      " 405\t0.000109\n",
      " 406\t0.000107\n",
      " 407\t0.000104\n",
      " 408\t0.000102\n",
      " 409\t0.000100\n",
      " 410\t0.000098\n",
      " 411\t0.000096\n",
      " 412\t0.000094\n",
      " 413\t0.000092\n",
      " 414\t0.000090\n",
      " 415\t0.000088\n",
      " 416\t0.000087\n",
      " 417\t0.000084\n",
      " 418\t0.000083\n",
      " 419\t0.000082\n",
      " 420\t0.000080\n",
      " 421\t0.000079\n",
      " 422\t0.000077\n",
      " 423\t0.000076\n",
      " 424\t0.000075\n",
      " 425\t0.000073\n",
      " 426\t0.000072\n",
      " 427\t0.000071\n",
      " 428\t0.000069\n",
      " 429\t0.000068\n",
      " 430\t0.000067\n",
      " 431\t0.000066\n",
      " 432\t0.000064\n",
      " 433\t0.000063\n",
      " 434\t0.000063\n",
      " 435\t0.000061\n",
      " 436\t0.000060\n",
      " 437\t0.000060\n",
      " 438\t0.000059\n",
      " 439\t0.000058\n",
      " 440\t0.000057\n",
      " 441\t0.000056\n",
      " 442\t0.000055\n",
      " 443\t0.000054\n",
      " 444\t0.000053\n",
      " 445\t0.000052\n",
      " 446\t0.000052\n",
      " 447\t0.000051\n",
      " 448\t0.000050\n",
      " 449\t0.000049\n",
      " 450\t0.000048\n",
      " 451\t0.000048\n",
      " 452\t0.000047\n",
      " 453\t0.000046\n",
      " 454\t0.000046\n",
      " 455\t0.000045\n",
      " 456\t0.000045\n",
      " 457\t0.000044\n",
      " 458\t0.000043\n",
      " 459\t0.000043\n",
      " 460\t0.000042\n",
      " 461\t0.000041\n",
      " 462\t0.000041\n",
      " 463\t0.000040\n",
      " 464\t0.000040\n",
      " 465\t0.000039\n",
      " 466\t0.000039\n",
      " 467\t0.000038\n",
      " 468\t0.000038\n",
      " 469\t0.000037\n",
      " 470\t0.000037\n",
      " 471\t0.000036\n",
      " 472\t0.000036\n",
      " 473\t0.000035\n",
      " 474\t0.000035\n",
      " 475\t0.000035\n",
      " 476\t0.000034\n",
      " 477\t0.000034\n",
      " 478\t0.000033\n",
      " 479\t0.000033\n",
      " 480\t0.000033\n",
      " 481\t0.000032\n",
      " 482\t0.000032\n",
      " 483\t0.000031\n",
      " 484\t0.000031\n",
      " 485\t0.000031\n",
      " 486\t0.000030\n",
      " 487\t0.000030\n",
      " 488\t0.000030\n",
      " 489\t0.000029\n",
      " 490\t0.000029\n",
      " 491\t0.000029\n",
      " 492\t0.000028\n",
      " 493\t0.000028\n",
      " 494\t0.000028\n",
      " 495\t0.000027\n",
      " 496\t0.000027\n",
      " 497\t0.000027\n",
      " 498\t0.000027\n",
      " 499\t0.000026\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y using operations on Tensors; these\n",
    "    # are exactly the same operations we used to compute the forward pass using\n",
    "    # Tensors, but we do not need to keep references to intermediate values since\n",
    "    # we are not implementing the backward pass by hand.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the a scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(\"%4d\\t%.6f\" % (t, loss))\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "    # of the loss with respect to w1 and w2 respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
    "    # Recall that tensor.data gives a tensor that shares the storage with\n",
    "    # tensor, but doesn't track history.\n",
    "    # You can also use torch.optim.SGD to achieve this.\n",
    "    with torch.no_grad():    # temporarily set all the requires_grad flag to false\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
