{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Defining New autograd Functions\n",
    "----------------------------------------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer and no biases, trained to\n",
    "predict y from x by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation computes the forward pass using operations on PyTorch\n",
    "Variables, and uses PyTorch autograd to compute gradients.\n",
    "\n",
    "In this implementation we implement our own custom autograd function to perform\n",
    "the ReLU function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\t54046960.000000\n",
      "   1\t55070600.000000\n",
      "   2\t46665268.000000\n",
      "   3\t27945894.000000\n",
      "   4\t12396962.000000\n",
      "   5\t5251271.000000\n",
      "   6\t2775569.000000\n",
      "   7\t1860413.250000\n",
      "   8\t1416878.375000\n",
      "   9\t1138690.500000\n",
      "  10\t937261.875000\n",
      "  11\t781948.562500\n",
      "  12\t658709.812500\n",
      "  13\t559323.062500\n",
      "  14\t478091.625000\n",
      "  15\t411089.218750\n",
      "  16\t355327.406250\n",
      "  17\t308593.875000\n",
      "  18\t269250.656250\n",
      "  19\t235910.250000\n",
      "  20\t207449.953125\n",
      "  21\t183025.984375\n",
      "  22\t161971.046875\n",
      "  23\t143749.812500\n",
      "  24\t127946.617188\n",
      "  25\t114168.718750\n",
      "  26\t102113.039062\n",
      "  27\t91527.234375\n",
      "  28\t82210.187500\n",
      "  29\t73987.578125\n",
      "  30\t66709.687500\n",
      "  31\t60248.187500\n",
      "  32\t54499.656250\n",
      "  33\t49374.042969\n",
      "  34\t44789.984375\n",
      "  35\t40686.312500\n",
      "  36\t37007.113281\n",
      "  37\t33701.589844\n",
      "  38\t30728.539062\n",
      "  39\t28048.916016\n",
      "  40\t25633.107422\n",
      "  41\t23452.644531\n",
      "  42\t21479.587891\n",
      "  43\t19691.175781\n",
      "  44\t18070.726562\n",
      "  45\t16598.136719\n",
      "  46\t15258.511719\n",
      "  47\t14038.626953\n",
      "  48\t12926.473633\n",
      "  49\t11911.927734\n",
      "  50\t10985.184570\n",
      "  51\t10136.625000\n",
      "  52\t9359.799805\n",
      "  53\t8648.249023\n",
      "  54\t7995.853027\n",
      "  55\t7398.795410\n",
      "  56\t6850.516602\n",
      "  57\t6346.312012\n",
      "  58\t5881.483887\n",
      "  59\t5453.844238\n",
      "  60\t5060.102539\n",
      "  61\t4697.192383\n",
      "  62\t4362.540527\n",
      "  63\t4054.453369\n",
      "  64\t3770.424561\n",
      "  65\t3508.355713\n",
      "  66\t3265.954590\n",
      "  67\t3041.769531\n",
      "  68\t2834.059570\n",
      "  69\t2641.658203\n",
      "  70\t2463.405518\n",
      "  71\t2298.046143\n",
      "  72\t2144.633545\n",
      "  73\t2002.208008\n",
      "  74\t1869.948730\n",
      "  75\t1747.055176\n",
      "  76\t1632.828979\n",
      "  77\t1526.585815\n",
      "  78\t1427.771851\n",
      "  79\t1335.814331\n",
      "  80\t1250.186279\n",
      "  81\t1170.400269\n",
      "  82\t1096.060181\n",
      "  83\t1026.746460\n",
      "  84\t962.193359\n",
      "  85\t901.967041\n",
      "  86\t845.738770\n",
      "  87\t793.236633\n",
      "  88\t744.203735\n",
      "  89\t698.391174\n",
      "  90\t655.570557\n",
      "  91\t615.520264\n",
      "  92\t578.070190\n",
      "  93\t543.025146\n",
      "  94\t510.230774\n",
      "  95\t479.527069\n",
      "  96\t450.776550\n",
      "  97\t423.848724\n",
      "  98\t398.617371\n",
      "  99\t374.967987\n",
      " 100\t352.796143\n",
      " 101\t332.006317\n",
      " 102\t312.502991\n",
      " 103\t294.205017\n",
      " 104\t277.030090\n",
      " 105\t260.912048\n",
      " 106\t245.775833\n",
      " 107\t231.564011\n",
      " 108\t218.209396\n",
      " 109\t205.665375\n",
      " 110\t193.874054\n",
      " 111\t182.791153\n",
      " 112\t172.370422\n",
      " 113\t162.572449\n",
      " 114\t153.356064\n",
      " 115\t144.682587\n",
      " 116\t136.522751\n",
      " 117\t128.842606\n",
      " 118\t121.612701\n",
      " 119\t114.804848\n",
      " 120\t108.394470\n",
      " 121\t102.356163\n",
      " 122\t96.666336\n",
      " 123\t91.307343\n",
      " 124\t86.256508\n",
      " 125\t81.495651\n",
      " 126\t77.008003\n",
      " 127\t72.774590\n",
      " 128\t68.784637\n",
      " 129\t65.021378\n",
      " 130\t61.470173\n",
      " 131\t58.121471\n",
      " 132\t54.960144\n",
      " 133\t51.977024\n",
      " 134\t49.162151\n",
      " 135\t46.503639\n",
      " 136\t43.993999\n",
      " 137\t41.624733\n",
      " 138\t39.386398\n",
      " 139\t37.273186\n",
      " 140\t35.276539\n",
      " 141\t33.390011\n",
      " 142\t31.607430\n",
      " 143\t29.922749\n",
      " 144\t28.331034\n",
      " 145\t26.826176\n",
      " 146\t25.403854\n",
      " 147\t24.058512\n",
      " 148\t22.786676\n",
      " 149\t21.584377\n",
      " 150\t20.446596\n",
      " 151\t19.371027\n",
      " 152\t18.353657\n",
      " 153\t17.390015\n",
      " 154\t16.479729\n",
      " 155\t15.617142\n",
      " 156\t14.801043\n",
      " 157\t14.029305\n",
      " 158\t13.298563\n",
      " 159\t12.606376\n",
      " 160\t11.951644\n",
      " 161\t11.331616\n",
      " 162\t10.744174\n",
      " 163\t10.188520\n",
      " 164\t9.661459\n",
      " 165\t9.162702\n",
      " 166\t8.690163\n",
      " 167\t8.242877\n",
      " 168\t7.818892\n",
      " 169\t7.417041\n",
      " 170\t7.036629\n",
      " 171\t6.675818\n",
      " 172\t6.333826\n",
      " 173\t6.010073\n",
      " 174\t5.702839\n",
      " 175\t5.411739\n",
      " 176\t5.136014\n",
      " 177\t4.874318\n",
      " 178\t4.626453\n",
      " 179\t4.391417\n",
      " 180\t4.168403\n",
      " 181\t3.956859\n",
      " 182\t3.756408\n",
      " 183\t3.566317\n",
      " 184\t3.385772\n",
      " 185\t3.214949\n",
      " 186\t3.052526\n",
      " 187\t2.898577\n",
      " 188\t2.752675\n",
      " 189\t2.614012\n",
      " 190\t2.482586\n",
      " 191\t2.357772\n",
      " 192\t2.239529\n",
      " 193\t2.127129\n",
      " 194\t2.020377\n",
      " 195\t1.919320\n",
      " 196\t1.823308\n",
      " 197\t1.732060\n",
      " 198\t1.645593\n",
      " 199\t1.563452\n",
      " 200\t1.485599\n",
      " 201\t1.411608\n",
      " 202\t1.341318\n",
      " 203\t1.274462\n",
      " 204\t1.211141\n",
      " 205\t1.151007\n",
      " 206\t1.093885\n",
      " 207\t1.039590\n",
      " 208\t0.988118\n",
      " 209\t0.939069\n",
      " 210\t0.892643\n",
      " 211\t0.848531\n",
      " 212\t0.806539\n",
      " 213\t0.766685\n",
      " 214\t0.728858\n",
      " 215\t0.692855\n",
      " 216\t0.658796\n",
      " 217\t0.626289\n",
      " 218\t0.595511\n",
      " 219\t0.566217\n",
      " 220\t0.538404\n",
      " 221\t0.511989\n",
      " 222\t0.486808\n",
      " 223\t0.462904\n",
      " 224\t0.440203\n",
      " 225\t0.418704\n",
      " 226\t0.398140\n",
      " 227\t0.378613\n",
      " 228\t0.360101\n",
      " 229\t0.342567\n",
      " 230\t0.325806\n",
      " 231\t0.309936\n",
      " 232\t0.294785\n",
      " 233\t0.280432\n",
      " 234\t0.266765\n",
      " 235\t0.253766\n",
      " 236\t0.241384\n",
      " 237\t0.229671\n",
      " 238\t0.218541\n",
      " 239\t0.207882\n",
      " 240\t0.197791\n",
      " 241\t0.188202\n",
      " 242\t0.179048\n",
      " 243\t0.170355\n",
      " 244\t0.162113\n",
      " 245\t0.154254\n",
      " 246\t0.146773\n",
      " 247\t0.139681\n",
      " 248\t0.132929\n",
      " 249\t0.126490\n",
      " 250\t0.120396\n",
      " 251\t0.114567\n",
      " 252\t0.109059\n",
      " 253\t0.103805\n",
      " 254\t0.098779\n",
      " 255\t0.094020\n",
      " 256\t0.089489\n",
      " 257\t0.085179\n",
      " 258\t0.081079\n",
      " 259\t0.077163\n",
      " 260\t0.073470\n",
      " 261\t0.069950\n",
      " 262\t0.066569\n",
      " 263\t0.063424\n",
      " 264\t0.060357\n",
      " 265\t0.057463\n",
      " 266\t0.054723\n",
      " 267\t0.052075\n",
      " 268\t0.049590\n",
      " 269\t0.047248\n",
      " 270\t0.044974\n",
      " 271\t0.042822\n",
      " 272\t0.040780\n",
      " 273\t0.038840\n",
      " 274\t0.036994\n",
      " 275\t0.035227\n",
      " 276\t0.033558\n",
      " 277\t0.031956\n",
      " 278\t0.030454\n",
      " 279\t0.029003\n",
      " 280\t0.027625\n",
      " 281\t0.026325\n",
      " 282\t0.025078\n",
      " 283\t0.023881\n",
      " 284\t0.022763\n",
      " 285\t0.021691\n",
      " 286\t0.020660\n",
      " 287\t0.019690\n",
      " 288\t0.018763\n",
      " 289\t0.017885\n",
      " 290\t0.017036\n",
      " 291\t0.016233\n",
      " 292\t0.015473\n",
      " 293\t0.014757\n",
      " 294\t0.014067\n",
      " 295\t0.013414\n",
      " 296\t0.012789\n",
      " 297\t0.012196\n",
      " 298\t0.011631\n",
      " 299\t0.011096\n",
      " 300\t0.010580\n",
      " 301\t0.010098\n",
      " 302\t0.009629\n",
      " 303\t0.009186\n",
      " 304\t0.008772\n",
      " 305\t0.008367\n",
      " 306\t0.007989\n",
      " 307\t0.007625\n",
      " 308\t0.007275\n",
      " 309\t0.006941\n",
      " 310\t0.006630\n",
      " 311\t0.006330\n",
      " 312\t0.006047\n",
      " 313\t0.005774\n",
      " 314\t0.005518\n",
      " 315\t0.005271\n",
      " 316\t0.005036\n",
      " 317\t0.004812\n",
      " 318\t0.004602\n",
      " 319\t0.004400\n",
      " 320\t0.004205\n",
      " 321\t0.004018\n",
      " 322\t0.003844\n",
      " 323\t0.003683\n",
      " 324\t0.003524\n",
      " 325\t0.003367\n",
      " 326\t0.003223\n",
      " 327\t0.003082\n",
      " 328\t0.002954\n",
      " 329\t0.002827\n",
      " 330\t0.002708\n",
      " 331\t0.002595\n",
      " 332\t0.002486\n",
      " 333\t0.002382\n",
      " 334\t0.002285\n",
      " 335\t0.002191\n",
      " 336\t0.002101\n",
      " 337\t0.002016\n",
      " 338\t0.001933\n",
      " 339\t0.001859\n",
      " 340\t0.001784\n",
      " 341\t0.001711\n",
      " 342\t0.001644\n",
      " 343\t0.001580\n",
      " 344\t0.001519\n",
      " 345\t0.001460\n",
      " 346\t0.001404\n",
      " 347\t0.001351\n",
      " 348\t0.001299\n",
      " 349\t0.001250\n",
      " 350\t0.001203\n",
      " 351\t0.001158\n",
      " 352\t0.001116\n",
      " 353\t0.001076\n",
      " 354\t0.001037\n",
      " 355\t0.000999\n",
      " 356\t0.000964\n",
      " 357\t0.000928\n",
      " 358\t0.000897\n",
      " 359\t0.000866\n",
      " 360\t0.000835\n",
      " 361\t0.000807\n",
      " 362\t0.000779\n",
      " 363\t0.000753\n",
      " 364\t0.000728\n",
      " 365\t0.000703\n",
      " 366\t0.000680\n",
      " 367\t0.000658\n",
      " 368\t0.000638\n",
      " 369\t0.000616\n",
      " 370\t0.000595\n",
      " 371\t0.000577\n",
      " 372\t0.000558\n",
      " 373\t0.000540\n",
      " 374\t0.000524\n",
      " 375\t0.000508\n",
      " 376\t0.000492\n",
      " 377\t0.000478\n",
      " 378\t0.000463\n",
      " 379\t0.000451\n",
      " 380\t0.000437\n",
      " 381\t0.000424\n",
      " 382\t0.000411\n",
      " 383\t0.000399\n",
      " 384\t0.000388\n",
      " 385\t0.000376\n",
      " 386\t0.000365\n",
      " 387\t0.000355\n",
      " 388\t0.000344\n",
      " 389\t0.000335\n",
      " 390\t0.000326\n",
      " 391\t0.000317\n",
      " 392\t0.000308\n",
      " 393\t0.000299\n",
      " 394\t0.000291\n",
      " 395\t0.000284\n",
      " 396\t0.000276\n",
      " 397\t0.000269\n",
      " 398\t0.000262\n",
      " 399\t0.000254\n",
      " 400\t0.000248\n",
      " 401\t0.000242\n",
      " 402\t0.000237\n",
      " 403\t0.000231\n",
      " 404\t0.000224\n",
      " 405\t0.000219\n",
      " 406\t0.000214\n",
      " 407\t0.000209\n",
      " 408\t0.000204\n",
      " 409\t0.000199\n",
      " 410\t0.000194\n",
      " 411\t0.000190\n",
      " 412\t0.000184\n",
      " 413\t0.000180\n",
      " 414\t0.000177\n",
      " 415\t0.000172\n",
      " 416\t0.000168\n",
      " 417\t0.000164\n",
      " 418\t0.000161\n",
      " 419\t0.000157\n",
      " 420\t0.000154\n",
      " 421\t0.000151\n",
      " 422\t0.000147\n",
      " 423\t0.000144\n",
      " 424\t0.000141\n",
      " 425\t0.000138\n",
      " 426\t0.000135\n",
      " 427\t0.000133\n",
      " 428\t0.000130\n",
      " 429\t0.000127\n",
      " 430\t0.000125\n",
      " 431\t0.000122\n",
      " 432\t0.000120\n",
      " 433\t0.000117\n",
      " 434\t0.000115\n",
      " 435\t0.000113\n",
      " 436\t0.000111\n",
      " 437\t0.000108\n",
      " 438\t0.000106\n",
      " 439\t0.000104\n",
      " 440\t0.000102\n",
      " 441\t0.000100\n",
      " 442\t0.000098\n",
      " 443\t0.000097\n",
      " 444\t0.000095\n",
      " 445\t0.000093\n",
      " 446\t0.000092\n",
      " 447\t0.000090\n",
      " 448\t0.000089\n",
      " 449\t0.000087\n",
      " 450\t0.000086\n",
      " 451\t0.000085\n",
      " 452\t0.000083\n",
      " 453\t0.000082\n",
      " 454\t0.000080\n",
      " 455\t0.000079\n",
      " 456\t0.000078\n",
      " 457\t0.000077\n",
      " 458\t0.000075\n",
      " 459\t0.000074\n",
      " 460\t0.000073\n",
      " 461\t0.000072\n",
      " 462\t0.000070\n",
      " 463\t0.000069\n",
      " 464\t0.000068\n",
      " 465\t0.000067\n",
      " 466\t0.000066\n",
      " 467\t0.000065\n",
      " 468\t0.000064\n",
      " 469\t0.000063\n",
      " 470\t0.000063\n",
      " 471\t0.000062\n",
      " 472\t0.000061\n",
      " 473\t0.000060\n",
      " 474\t0.000059\n",
      " 475\t0.000058\n",
      " 476\t0.000057\n",
      " 477\t0.000056\n",
      " 478\t0.000055\n",
      " 479\t0.000055\n",
      " 480\t0.000054\n",
      " 481\t0.000053\n",
      " 482\t0.000053\n",
      " 483\t0.000052\n",
      " 484\t0.000051\n",
      " 485\t0.000051\n",
      " 486\t0.000050\n",
      " 487\t0.000049\n",
      " 488\t0.000049\n",
      " 489\t0.000048\n",
      " 490\t0.000047\n",
      " 491\t0.000047\n",
      " 492\t0.000046\n",
      " 493\t0.000046\n",
      " 494\t0.000045\n",
      " 495\t0.000045\n",
      " 496\t0.000044\n",
      " 497\t0.000044\n",
      " 498\t0.000044\n",
      " 499\t0.000043\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# Create random Tensors for weights.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # ReLU using our custom autograd operation.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(\"%4d\\t%.6f\" % (t, loss))\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
